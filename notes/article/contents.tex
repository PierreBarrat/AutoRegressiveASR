% !TEX root = main.tex

\subsection{Introduction}

\subsection{Results} % (fold)
\label{sub:results}

\subsubsection{Autoregressive model of sequence evolution} 
\label{sub:autoregressive_sequence_evolution_model}

Models of evolution commonly used in phylogenetics rely on the assumptions that sequence positions evolve independently and that evolution at each position $i$ follows a continuous time Markov chain (CTMC) parametrized by a substitution rate matrix $\Qmat^i$. 
Matrix $\Qmat^i$ is of dimensions $q \times q$ where $q=4$ for DNA, $20$ for amino acids or $64$ for codon models.
The probability of observing a change from state $a$ to state $b$ during evolutionary time $t$ is then given by $P(b \vert a, t) = \left( e^{t\Qmat^i} \right)_{ab}$.

If the model is time-reversible, it is a general property of CTMCs that the substitution rate matrix can be written as 

\begin{equation}
	\label{eq:Q_decomposition}
	\Qmat = \mathbf{H}\cdot \mathbf{\Pi} = \mathbf{H} \cdot \begin{pmatrix}
		\pi_1 & 0 & 0\\ 
		0 & \ddots & 0 \\
		0 & 0 & \pi_q
	\end{pmatrix},
\end{equation}

where $\mathbf{H}$ is symmetric and $\mathbf{\Pi}$ is diagonal with entries that sum to 1 \cite{yang_computationalmolecularevolution_2006}. 
The two matrices have simple interpretations. 
On the first hand, $\mathbf{\Pi}$ fixes the long-term equilibrium frequencies, that is $P(b \vert a, t) \xrightarrow[t \rightarrow \infty]{} \pi_b$.
On the other, $\mathbf{H}$ influences the dynamics of the Markov chain but does not change the equilibrium distribution. 
Most commonly, $\mathbf{\Pi}$ is considered to be independent of the sequence position $i$, while $\mathbf{H}$ can be multiplied by position-dependent rates in order to model the different variability of different sites \cite{yang_maximumlikelihoodphylogenetic_1994,stamatakis_raxmlversiontool_2014,nguyen_iqtreefasteffective_2015}. \\


In order to incorporate constraints coming from a protein's structure and function into the evolutionary model, we develop a family-specific model of protein sequence evolution based on the the autoregressive generative model ArDCA \cite{trinquier_efficientgenerativemodeling_2021}. 
ArDCA models the diversity of sequences in a protein family using a set of learned conditional probabilities. 
In practice, the model assigns a probability to any sequence $\mathbf{a} = \{a_1, \ldots, a_L\}$ of $L$ amino acids: 

\begin{equation}
	\label{eq:autoregressive_def}
	P^{AR}(\mathbf{a}) = \prod_{i=1}^L p_i(a_i \vert a_{<i}),
\end{equation}

where the product runs over positions in the sequence and $a_{<i} = a_1, \ldots, a_{i-1}$ represents all amino acids before position $i$. 
Functions $p_i$ represent the probability according to the model to observe state $a_i$ in position $i$, given that the previous amino acids were $a_1, \ldots, a_{i-1}$. 
Their precise functional form is given in the methods section. 
They are learned using the aligned sequences of members of the family. 
In actual implementations, the order in which the product in \eqref{eq:autoregressive_def} is performed is not the natural $(1, \ldots, L)$ but rather an order where positions are sorted by increasing variability.
This does not significantly effect the model we present below, and we keep the notation of \eqref{eq:autoregressive_def} for simplicity. 

It has been shown in \cite{trinquier_efficientgenerativemodeling_2021} that the generative capacities of ArDCA are comparable to that of state of the art models such as bmDCA \cite{mcgee_generativecapacityprobabilistic_2021}. 
This means that a set of sequences sampled from the probability in \eqref{eq:autoregressive_def} is statistically hard to distinguish from the natural sequences used in training or, in other words, that the model can be used to sample new artificial homologs of a protein family. 
Generative capacities of a protein model come from its ability to represent epistasis, that is the relation between the effect of a mutation and sequence context in which it occurs. 
Here, epistasis is modeled through the conditional probabilities $p_i$: the distribution of amino acids at position $i$ depends on the states of the previous positions $1, \ldots i-1$. 


We take advantage of the autoregressive architecture to define the following evolution model. 
Given two amino acid sequences $\mathbf{a}$ and $\mathbf{b}$, we propose

\begin{equation}
	\label{eq:autoregressive_propagator_full}
	P(\mathbf{b} \vert \mathbf{a}, t) = \prod_{i=1}^L q_i(b_i \vert a_i, b_{<i}, t),
\end{equation}

where the conditional propagator $q_i$ is defined as 

\begin{equation}
	\label{eq:autoregressive_site_propagator}
	q_i(b_i \vert a_i, b_{<i}, t) = \left(e^{t \cdot Q^i(b_{<i})}\right)_{a_i, b_i}, 
	\quad Q^i(b_{<i}) = \mathbf{H} \cdot \begin{pmatrix}
		p_i(1 \vert b_{<i}) & 0 & 0\\ 
		0 & \ddots & 0 \\
		0 & 0 & p_i(q \vert b_{<i})
	\end{pmatrix}. 
\end{equation}

According to these equations, evolution for each position $i$ follows a standard CTMC. 
However, we use the decomposition of \eqref{eq:Q_decomposition} to set the equilibrium frequency at $i$ to $p_i(b \vert b_{<i})$. 
In other words, we consider that position $i$ evolves in the context of $b_1, \ldots, b_{i-1}$, and that its dynamics are constrained by its long term frequency given by the autoregressive model. 
An important consequence of this choice is that our evolution model will converge at long times to the generative distribution $P^{AR}$: 

\begin{equation}
	\label{eq:autoregressive_long_term}
	q_i(b_i \vert a_i, b_{<i}, t) \xrightarrow[t \rightarrow \infty]{} p_i(b_i \vert b_{<i}), 
	\;\; P(\mathbf{b} \vert \mathbf{a}, t) \xrightarrow[t \rightarrow \infty]{} P^{AR}(\mathbf{b}).
\end{equation}

We argue here that such a property is essential to build a realistic protein sequence evolution model, particularly when considering evolution over a relatively long time frame. 
Note that to converge to a generative distribution, accurate modeling of epistasis is required. 
Using site-specific frequencies would not be sufficient, as the effect of mutations in a protein sequence typically depends on the context \cite{socolich_evolutionaryinformationspecifying_2005}. 
The technique proposed here allows us to represent epistasis through the context dependent probabilities $p_i$, while still considering each sequence position one at a time. 

Interestingly, we note that the model in \eqref{eq:autoregressive_propagator_full} is not time reversible, although context dependent site propagators in \eqref{eq:autoregressive_site_propagator} are reversible. 
We show in the SM that this is mainly an artifact of the autoregressive nature of the model coupled with epistasis. 
Using non-time reversible evolutionary models is uncommon in the field, but this is mainly due to practical considerations and there are no fundamental reasons for evolution itself to be reversible \cite{felsensteinjoseph_inferringphylogenies_2003}. 
In practice, this means that algorithms using this model have to be adapted accordingly. \\


We underline that this approach has important differences with standard models of evolution used in phylogenenetics. 
In phylogenetic reconstruction, the tree and the sequence evolution model are usually inferred at the same time and from the same data. 
The number of parameters of the evolution model is then kept low to reduce the risk of overfitting, for instance by using site specific rates to account for variable and conserved sites. 
Methods that introduce more complex models such as site specific frequencies do so by jointly inferring the parameters and the tree, leading to relatively complex algorithms \cite{halpern_evolutionarydistancesproteincoding_1998a,puller_efficientinferencepotential_2020}.

Here instead, parameters of the generative model in \eqref{eq:autoregressive_def} are learned from a protein family, \emph{i.e.} a set of diverged homologous protein sequences. 
While it is true that these sequences share a common evolutionary history and cannot be considered as independent samples, common learning procedures only account for this in a very crude way \cite{cocco_inversestatisticalphysics_2018,trinquier_efficientgenerativemodeling_2021}.
Despite this, it appears that the generative properties of such models are not strongly affected by ignoring the phylogeny \cite{hockenberry_phylogeneticweightingdoes_2019,rodriguezhorta_effectphylogeneticcorrelations_2021}. 
This allows us to proceed in two steps: first construct the model from data while ignoring phylogeny, and then only use it for phylogenetic inference tasks.

An advantage of this approach is that once the model of \eqref{eq:autoregressive_def} is inferred, the propagator in \eqref{eq:autoregressive_propagator_full} comes ``for free'' as no additional parameters are required. 
Importantly, our model does not use site specific substitution rates. 
Indeed, it has been shown that these can be seen as emergent properties when using more complex evolution models such as the one presented here \cite{delapaz_epistaticcontributionspromote_2020}.
However, a disadvantage is that the technique is only applicable to a given protein family at a time, and requires the existence of an appropriate training set for the model. 


\subsubsection{Ancestral sequence reconstruction}

We illustrate the advantages of the proposed sequence evolution model by applying it to the task of ancestral sequence reconstruction (ASR). 
The goal of ASR is the following: given a set of extant sequences with a shared evolutionary history and the corresponding phylogenetic tree, is it possible to reconstruct the sequences of extinct ancestors at the internal nodes of the tree? 
One faces two difficulties when evaluating the capacity of a model to perform ASR. 
The first is that in the case of biological data, the real phylogeny and ancestral sequences are not usually not known. 
As a consequence, one must rely on simulated data. 
The second is that for internal nodes that are sufficiently remote from the leaves, reconstruction of the exact ancestor is a hopeless task as the uncertainty then becomes high. 
This means that it is only possible to make a statistical assessment about the quality of a reconstruction. 

To test our approach, we adopt the following setup. 
We first generate phylogenetic trees by sampling from a coalescent process.
We decide to use Yule's coalescent instead of the more common Kingman.
The latter tends to produce a large majority of internal nodes in close vicinity to the leaves with the others separated by very long branches, resulting in a trivial reconstruction for most nodes and a very hard one for the deep nodes. 
Yule's coalescent generates a more even distribution of node depths, allowing us to better evaluate reconstruction quality, see SM and Figure~\sref{sfig:depth_distribution_coalescent}.
For each tree, we simulate the evolution of sequences using a ``forward'' model to obtain two multiple sequence alignments, one for the leaves and one for the internal nodes of the tree.
We then reconstruct internal nodes using the desired approach by using the leaf alignment and the tree topology as input data. 

In the case of our autoregressive approach, we proceed as follows: 
\begin{enumerate}[label=\emph{\roman*}]	
	\item for sequence position $i=1$, use the evolution model defined by the equilibrium frequencies $p_1$ to reconstruct a state $a^n_1$ at each internal node $n$ of the tree;
	\item iterating through subsequent positions $i > 1$: reconstruct state $a^n_i$ at each internal node $n$ using the model defined in \eqref{eq:autoregressive_site_propagator}, with the context $a^n_{<i}$ having been already reconstructed in a previous iteration. 
\end{enumerate}
It is important to note that when any position $i>1$ is reconstructed, the context at different internal nodes of the tree may differ. 
For a branch joining two nodes $(n, m)$ of the tree, the evolution model will thus differ if we go down or up the branch: in one case the context at node $n$ must be used, in the other case the context at node $m$. 
This is a consequence of the time-irreversibility of the model. 
For this reason, we use a variant of Felsenstein's pruning algorithm that is adapted to irreversible models \cite{boussau_efficientlikelihoodcomputations_2006}. 
This comes at no computational cost. 

As a comparison, we also reconstruct ancestral sequences using IQ-TREE \cite{minh_iqtreenewmodels_2020}. 
Both methods run on a fixed tree topology, with branch lengths being re-inferred using maximum likelihood (see Methods). 



\subsection{Discussion}

\subsection{Methods} % (fold)
\label{sub:methods}

\subsubsection{ArDCA}

The ArDCA model assigns a probability to any sequence of amino acids of length $L$ given by 

\begin{equation}
	\label{eq:autoregressive_def_methods}
	P^{AR}(\mathbf{a}) = \prod_{i \in \sigma(L)} p_i(a_i \vert a_{<i}),
\end{equation}

where $\sigma(L)$ is a permutation of the $L$ first integers and $a_{<i}$ stands for $a_1, \ldots, a_{i-1}$. 
This means that the order in which the conditional probabilities $p_i$ are applied is not necessarily the sequence order. 
The permutation $\sigma$ is fixed at model inference. 

Conditional probabilities $p_i$ are defined as

\begin{equation}
	p_i(a_i \vert a_{<i}) = \frac{1}{Z_i}\exp\left( \sum_{j < i} J_{ij}(a_i, a_j) + h_i(a_i) \right),
\end{equation}

with the $i$ $q$-dimensional vectors $J_{i\cdot}$ and $h_i$ are learned parameters. 
It was shown in \cite{trinquier_efficientgenerativemodeling_2021} that such a parametrization captures essential features of the variability of members of a proteins family. 

\subsubsection{Simulations}

A simulation is performed as follows. 
First, a random tree of $n=100$ leaves is generated from Yule's coalescent. 
We then normalize its height to a fixed value $H$ that depends on the forward model used: for the autoregressive model we use $H=2.0$, while for the Potts model combined with Metropolis steps, we use $H=8$ sweeps, \emph{i.e.} $H=8 L$ Metropolis steps where $L$ is the length of the sequences. 

A root sequence is sampled from the forward model's equilibrium distribution, and evolution is simulated along each branch independently starting from the root. 
In this way, we obtain for each repetition a tree and the alignments for internal and leaf nodes. 
Results presented in this work are obtained by averaging over $M=100$ such simulations for each protein family. 

\begin{itemize}
	\item proper presentation of ardca (sequence ordering, functional form of cond probs)
	\item details of simulations: number of leaves, size of trees, etc...
	\item how I run iqtree + results of model finder
	\item how branch lengths are reconstructed 
\end{itemize}

